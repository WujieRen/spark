2018/1/10:
    1. 换新版本后，运行MockData时总是报错说Incompatibe Version Jackson。注释掉后可以运行了。
        ——很重要的一点是为什么要用这个？ 如果不知到为什么要用，在哪里用，写了又有什么意义？

    2. 在spark2.0后，DataFrame的API和DataSet的API合并统一了，现在只需要处理DataSet相关API即可。Reference：
        https://www.iteblog.com/archives/1566.html#DataFrame

        http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html

    3.关于遍历DataSet那块儿，找半天资料找不到。最后在DataSet官方API文档中找到适合的方法。
        ——可能引发的一个学习方法：去官方APi文档中了解一下大概的方法有哪些。

    4.Dataset保存到本地。
        http://spark.apachecn.org/docs/cn/2.2.0/sql-programming-guide.html#manually-specifying-options-手动指定选项


2018/1/11

    1.
        JdbcCRUD(原理、实现、测试)

    2.
        单例设计模式(原理、实现)；
            单例模式Demo
             *
             * 单例模式是指：当不希望外界随意创建 xxx.类 的实例，在整个程序运行期间，只有一个实例。
             *
             * 要实现单例模式，几个要点：
             *   1、如果不希望外界随意创建该类的对象，那么 Constructor 必须是 private 的。
             *   2、既然 Constructor 被 private 了，那么外界就只能通过 static 的方法去获取。
             *      所以类必须提供要给 static function， 通常是 getInstance()，
             *   3、且 getInstance() 能够保证类的实例只能被创建一次，返回唯一的一个实例。
             *
             * 单例模式应用场景：
             *   1、配置管理组件。可以在读取大量配置信息后，用单例模式，将配置信息仅仅保存在一个实例变量中，全局就只有一个实例。
             *      这样可以避免对于静态不变的配置信息的反复读取。
             *   2、JDBC辅助组件。全局只有一个实例，实例中持有一个简单的内部数据源。
             *      单例模式可以保证全局只有一个实例，那么数据源也只有一个，这样就不会重复创建数据源了(数据库连接池)。
        多线程；
        Jdbc辅助组件
            //TODO:什么时候需要考虑多线程？ —— http://blog.csdn.net/dsc2015/article/details/52848208
            //TODO:匿名内部类和回掉函数没弄明白

            批量执行SQL语句
                 *
                 * 批量执行SQL语句，是JDBC中的一个高级功能
                 * 默认情况下，每次执行一条SQL语句，就会通过网络连接，向MySQL发送一次请求
                 *
                 * 但是，如果在短时间内要执行多条结构完全一模一样的SQL，只是参数不同
                 * 虽然使用PreparedStatement这种方式，可以只编译一次SQL，提高性能，但是，还是对于每次SQL
                 * 都要向MySQL发送一次网络请求
                 *
                 * 可以通过批量执行SQL语句的功能优化这个性能
                 * 一次性通过PreparedStatement发送多条SQL语句，比如100条、1000条，甚至上万条
                 * 执行的时候，也仅仅编译一次就可以
                 * 这种批量执行SQL语句的方式，可以大大提升性能


2018/1/12
    1.
        JdbcHelperTest
            ① 熟悉了打断点Debug，并成功修复问题
            ②
              /**
               * 设计内部接口QueryCallback的用意：
               *  1.在执行sql语句时，可以封装和指定自己查询结果的处理逻辑
               *  2.封装在一个内部接口的匿名内部类对象中，传入JdbcHelper的方法，
               *  3.在方法内部，可以回掉自定义的逻辑，处理查询结果，并将结果放入外部的变量中
               */

    2.
        Dao
            ① 实体类用基本类型好还是包装类型好？
            	https://www.cnblogs.com/rocky-AGE-24/p/5944278.html

            ② 深入理解 final 关键字：
            	https://www.cnblogs.com/dolphin0520/p/3736238.html

    3. Spark2.0后SparkSession入口取代了原本的SQLContext与HiveContext。所以重构了一下Session分析的入口 && 模拟数据的Spark入口。
        -》中间遇到一个错误：Invalid Integer ...
        -》最终原因是：userId 定义时定义的int，而在Schema中定义成了LongType，导致数据类型不匹配。(找了三个小时多...最终找罗晶解决的。他的解决方法：他在那个 DataSet形成的地方打了各断点，一个一个数据类型对照，最终解决。)

    4.编码过程中要清楚地知道每一步数据处理操作的来源和结果数据长什么样子，每个元素是什么类型。这个很重要。

    5.按session粒度聚合数据完成。
        数据结构：Tuple2<String, String>(sessionId, fullAggrInfo)

        ① row.getLong()、row.geFloat()等方法当遇到控制时会报错。我用-1处理了。
        ② Debug小经验：Debug时可以再数据处理形成新的数据的地方打断点，看哪里出问题了。


2018/1/14
    1.按筛选参数过滤聚合session数据 && 测试通过
        结果：
            989 ----------------------------(7fbee8e9cbb84a1587882cf6e979e11e,sessionid=7fbee8e9cbb84a1587882cf6e979e11e|searchKeywords=|clickCategoryIds=|age=5|professional=profession_49|city=city_185|sex=男)
            108 ----------------------------(7644fd6e87f9405fbe0a1b1972d7f4be,sessionid=7644fd6e87f9405fbe0a1b1972d7f4be|searchKeywords=串串|clickCategoryIds=|age=42|professional=profession_58|city=city_110|sex=女)
        问题：
            1、//TODO:final 过滤时没有用final修饰parameter也不影响，为什么还要定义成final?
            2、过滤数目不对，查找原因为ValidUtils的between方法定义有问题：
                ①在paramter中查找的参数去data中查找了。
                ②比较大小时定义成了只要小于endAge的就可以通过。
                这两个属于不细心 —— 或者说脑子思路不够清晰

    2.自定义计数器
        Spark2.0.0以后更新为Accumulator2

    3.重构思路 && 重构session聚合

    4.重构session过滤 && 过滤后对session个数，各访问时长、访问步长范围进行计数
        Spark2.0后Accumulator也有更新：由原来的AccumulatorParam -> AccumulatorV2

        reference:
            http://www.cnblogs.com/zhangweilun/p/6684776.html
            http://blog.csdn.net/duan_zhihua/article/details/75269994

        问题：
            ①主要是merge()的功能一开始没弄明白。merge()其实是当前result和other.result的一个累加操作，即合并。
            第一篇reference的merge()逻辑定义有误。

        补充：
            对于Accumulator这种分布式累加计算变量的使用，从Accumulator中获取数据，一定要在某个action操作之后进行。如果没有action操作的话，整个程序根本不会运行。

            另外，必须要把能够触发job执行的操作，放在最终写入MySQL方法之前。

2018/1/17
    问题一：百分百比计算有误
        原因：业务没理解清楚
        解决：找半天找不到问题，最终发现是业务理解有误。访问时长统计 和 步长统计 是互相独立的。两者算的是分别占比。总和应该是2。(妈个鸡...从昨晚弄到现在，找了7个小时发现没bug...)


    问题二：随机抽取数据插入无效
        解决： 问题在于没有action动作触发那个RDD执行，导致flatMapToPair()方法没有执行。

    问题三：session_random_extract表中，category_ids全是NULL
        原因：修改时没有把相关有需要代码都改掉。
        解决： 在MockData时，前面把 clickCategoryId 的值通过替换成-1L来解决row.getLong()报 NullPointerException 。结果在mockData时忘记把筛选条件改掉了，导致不会往 clickCategoryId 中放任何值。

    问题四：local[*]报错
        原因：SimpleDateFormat是非线程安全类，多线程调用时报错。
        解决：synchronized
2018/1/18
    问题一：往sexxion_random_extract写数据时速度极慢，且数量不正确。
        //TODO:解决：换了一下...的位置。为什么还一下这个位置效果这么不一样？？？

    问题二：想注释掉无意义的控制台输出，结果不能注释掉两个mockData中的df.collect，具体原因不明。
        //TODO:原因：原因不明。

    问题三：Exception in thread "main" java.lang.NumberFormatException: Infinite or NaN
        原因：除数(session_count)为0。怎么会是0 呢？???——没调用该job。

    问题四：session_count的个数会比filter..RDD多一条，why???
        //TODO:原因不明

    问题五：按照每天应该取100，为何最后总是不够100，差8条左右。
        原因：double强转int会把小数点后的截掉。而不是四舍五入。







