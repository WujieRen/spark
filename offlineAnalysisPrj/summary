2018/1/10:
    1. 换新版本后，运行MockData时总是报错说Incompatibe Version Jackson。注释掉后可以运行了。
        ——很重要的一点是为什么要用这个？ 如果不知到为什么要用，在哪里用，写了又有什么意义？

    2. 在spark2.0后，DataFrame的API和DataSet的API合并统一了，现在只需要处理DataSet相关API即可。Reference：
        https://www.iteblog.com/archives/1566.html#DataFrame

        http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html

    3.关于遍历DataSet那块儿，找半天资料找不到。最后在DataSet官方API文档中找到适合的方法。
        ——可能引发的一个学习方法：去官方APi文档中了解一下大概的方法有哪些。

    4.Dataset保存到本地。
        http://spark.apachecn.org/docs/cn/2.2.0/sql-programming-guide.html#manually-specifying-options-手动指定选项


2018/1/11

    1.
        JdbcCRUD(原理、实现、测试)

    2.
        单例设计模式(原理、实现)；
            单例模式Demo
             *
             * 单例模式是指：当不希望外界随意创建 xxx.类 的实例，在整个程序运行期间，只有一个实例。
             *
             * 要实现单例模式，几个要点：
             *   1、如果不希望外界随意创建该类的对象，那么 Constructor 必须是 private 的。
             *   2、既然 Constructor 被 private 了，那么外界就只能通过 static 的方法去获取。
             *      所以类必须提供要给 static function， 通常是 getInstance()，
             *   3、且 getInstance() 能够保证类的实例只能被创建一次，返回唯一的一个实例。
             *
             * 单例模式应用场景：
             *   1、配置管理组件。可以在读取大量配置信息后，用单例模式，将配置信息仅仅保存在一个实例变量中，全局就只有一个实例。
             *      这样可以避免对于静态不变的配置信息的反复读取。
             *   2、JDBC辅助组件。全局只有一个实例，实例中持有一个简单的内部数据源。
             *      单例模式可以保证全局只有一个实例，那么数据源也只有一个，这样就不会重复创建数据源了(数据库连接池)。
        多线程；
        Jdbc辅助组件
            //TODO:什么时候需要考虑多线程？ —— http://blog.csdn.net/dsc2015/article/details/52848208
            //TODO:匿名内部类和回掉函数没弄明白

            批量执行SQL语句
                 *
                 * 批量执行SQL语句，是JDBC中的一个高级功能
                 * 默认情况下，每次执行一条SQL语句，就会通过网络连接，向MySQL发送一次请求
                 *
                 * 但是，如果在短时间内要执行多条结构完全一模一样的SQL，只是参数不同
                 * 虽然使用PreparedStatement这种方式，可以只编译一次SQL，提高性能，但是，还是对于每次SQL
                 * 都要向MySQL发送一次网络请求
                 *
                 * 可以通过批量执行SQL语句的功能优化这个性能
                 * 一次性通过PreparedStatement发送多条SQL语句，比如100条、1000条，甚至上万条
                 * 执行的时候，也仅仅编译一次就可以
                 * 这种批量执行SQL语句的方式，可以大大提升性能


2018/1/12
    1.
        JdbcHelperTest
            ① 熟悉了打断点Debug，并成功修复问题
            ②
              /**
               * 设计内部接口QueryCallback的用意：
               *  1.在执行sql语句时，可以封装和指定自己查询结果的处理逻辑
               *  2.封装在一个内部接口的匿名内部类对象中，传入JdbcHelper的方法，
               *  3.在方法内部，可以回掉自定义的逻辑，处理查询结果，并将结果放入外部的变量中
               */

    2.
        Dao
            ① 实体类用基本类型好还是包装类型好？
            	https://www.cnblogs.com/rocky-AGE-24/p/5944278.html

            ② 深入理解 final 关键字：
            	https://www.cnblogs.com/dolphin0520/p/3736238.html

    3. Spark2.0后SparkSession入口取代了原本的SQLContext与HiveContext。所以重构了一下Session分析的入口 && 模拟数据的Spark入口。
        -》中间遇到一个错误：Invalid Integer ...
        -》最终原因是：userId 定义时定义的int，而在Schema中定义成了LongType，导致数据类型不匹配。(找了三个小时多...最终找罗晶解决的。他的解决方法：他在那个 DataSet形成的地方打了各断点，一个一个数据类型对照，最终解决。)

    4.编码过程中要清楚地知道每一步数据处理操作的来源和结果数据长什么样子，每个元素是什么类型。这个很重要。

    5.按session粒度聚合数据完成。
        ① row.getLong()、row.geFloat()等方法当遇到控制时会报错。我用-1处理了。
        ② Debug时可以再数据处理形成新的数据的地方打断点，看哪里出问题了。



